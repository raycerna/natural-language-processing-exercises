{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c9cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "from time import strftime\n",
    "\n",
    "import acquire\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16f830",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "#### The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "#### In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "    - Lowercase everything\n",
    "    - Normalize unicode characters\n",
    "    - Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "    - This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n",
    "\n",
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.\n",
    "\n",
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n",
    "\n",
    "8. For each dataframe, produce the following columns:\n",
    "\n",
    "    - title to hold the title\n",
    "    - original to hold the original article/post content\n",
    "    - clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "    - stemmed to hold the stemmed version of the cleaned data.\n",
    "    - lemmatized to hold the lemmatized version of the cleaned data.\n",
    "\n",
    "9. Ask yourself:\n",
    "\n",
    "    - If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    - If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    - If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd7b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define a basic_clean function for a single document (one string)\n",
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    # we will normalize our data into standard NFKD unicode, feed it into an ascii encoding\n",
    "    # decode it back into UTF-8\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    # utilize our regex substitution to remove our undesirable characters, then lowercase\n",
    "    string = re.sub(r\"[^\\w0-9'\\s]\", '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0b8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # make our tokenizer, taken from nltk's ToktokTokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    # apply our tokenizer's tokenization to the string being input, ensure it returns a string\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb4307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # create our stemming object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # use a list comprehension => stem each word for each word inside of the entire document,\n",
    "    # split by the default, which are single spaces\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    # glue it back together with spaces, as it was before\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7cea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # create our lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # use a list comprehension to lemmatize each word\n",
    "    # string.split() => output a list of every token inside of the document\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    # glue the lemmas back together by the strings we split on\n",
    "    string = ' '.join(lemmas)\n",
    "    #return the altered document\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad135655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2, 3, 4]\n",
    "list2 = [2, 1, 3, 4]\n",
    "\n",
    "print(set(list1)==set(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad76952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'c', 'd'] {'b', 'a', 'c', 'd'}\n"
     ]
    }
   ],
   "source": [
    "mylist = ['a', 'b', 'c', 'c', 'd']\n",
    "\n",
    "myset = set(mylist)\n",
    "\n",
    "print(mylist, myset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6155396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # assign our stopwords from nltk into stopword_list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # utilizing set casting, i will remove any excluded stopwords\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    # add in any extra words to my stopwords set using a union\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    # split our document by spaces\n",
    "    words = string.split()\n",
    "    # every word in our document, as long as that word is not in our stopwords\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    # glue it back together with spaces, as it was so it shall be\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    # return the document back\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b3fc986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Jemison', 'How', 'are', 'you']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = ['Hello', 'Jemison', 'How', 'are', 'you']\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d959625e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello/Jemison/How/are/you'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a78f6f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter wins bid to fast-track trial over Musk...</td>\n",
       "      <td>Twitter's lawsuit seeking to hold Elon Musk to...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lyricist Jaani injured in accident in Punjab, ...</td>\n",
       "      <td>Renowned Punjabi lyricist and music composer J...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rare twin giraffes born in Kenya, pics surface</td>\n",
       "      <td>Rare twin giraffes have been born at Nairobi N...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She borrowed mother's stole: Father of girl al...</td>\n",
       "      <td>The father of a 17-year-old who claimed she wa...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi-bound flight rejected take-off due to do...</td>\n",
       "      <td>A flight was rejected take-off from Leh due to...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>NIA raids 3 Andhra locations in Chhattisgarh N...</td>\n",
       "      <td>National Investigation Agency (NIA) conducted ...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Strikes on Syria will destabilise Middle East:...</td>\n",
       "      <td>Iranian Supreme Leader Ayatollah Ali Khamenei ...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Turkey court upholds exit from key European tr...</td>\n",
       "      <td>A Turkish court ruled on Tuesday that Presiden...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>France probes origins of wildfire, 1 detained</td>\n",
       "      <td>French investigators detained a 39-year-old ma...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Woman sleepwalks through window in Germany, fa...</td>\n",
       "      <td>An 18-year-old woman in Germany's Neubrandenbu...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Twitter wins bid to fast-track trial over Musk...   \n",
       "1    Lyricist Jaani injured in accident in Punjab, ...   \n",
       "2       Rare twin giraffes born in Kenya, pics surface   \n",
       "3    She borrowed mother's stole: Father of girl al...   \n",
       "4    Delhi-bound flight rejected take-off due to do...   \n",
       "..                                                 ...   \n",
       "295  NIA raids 3 Andhra locations in Chhattisgarh N...   \n",
       "296  Strikes on Syria will destabilise Middle East:...   \n",
       "297  Turkey court upholds exit from key European tr...   \n",
       "298      France probes origins of wildfire, 1 detained   \n",
       "299  Woman sleepwalks through window in Germany, fa...   \n",
       "\n",
       "                                               content    category  \n",
       "0    Twitter's lawsuit seeking to hold Elon Musk to...    national  \n",
       "1    Renowned Punjabi lyricist and music composer J...    national  \n",
       "2    Rare twin giraffes have been born at Nairobi N...    national  \n",
       "3    The father of a 17-year-old who claimed she wa...    national  \n",
       "4    A flight was rejected take-off from Leh due to...    national  \n",
       "..                                                 ...         ...  \n",
       "295  National Investigation Agency (NIA) conducted ...  automobile  \n",
       "296  Iranian Supreme Leader Ayatollah Ali Khamenei ...  automobile  \n",
       "297  A Turkish court ruled on Tuesday that Presiden...  automobile  \n",
       "298  French investigators detained a 39-year-old ma...  automobile  \n",
       "299  An 18-year-old woman in Germany's Neubrandenbu...  automobile  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = acquire.get_news_articles_data()\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f469d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>Have you been considering a career in Cloud Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>If you are interested in embarking on a career...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Our Cloud Administration Program Right for ...</td>\n",
       "      <td>Changing careers can be scary. The first thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 Reasons To Attend Our New Cloud Administrati...</td>\n",
       "      <td>Come Work In The Cloud\\nWhen your Monday rolls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>Have you been considering a career in Cloud Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>If you are interested in embarking on a career...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In-Person Workshop: Learn to Code – JavaScript...</td>\n",
       "      <td>Join us for our live in-person JavaScript cras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In-Person Workshop: Learn to Code – Python on ...</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Free JavaScript Workshop at Codeup Dallas on 6/28</td>\n",
       "      <td>Event Info: \\nLocation – Codeup Dallas\\nTime –...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is Our Cloud Administration Program Right for ...</td>\n",
       "      <td>Changing careers can be scary. The first thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PRIDE in Tech Panel</td>\n",
       "      <td>In celebration of PRIDE month, join our Codeup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Inclusion at Codeup During Pride Month (and Al...</td>\n",
       "      <td>Happy Pride Month! Pride Month is a dedicated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mental Health First Aid Training</td>\n",
       "      <td>As a student of Codeup, going through a massiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Codeup Dallas: How to Succeed at a Coding Boot...</td>\n",
       "      <td>This event is the perfect opportunity for peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5 Reasons To Attend Our New Cloud Administrati...</td>\n",
       "      <td>Come Work In The Cloud\\nWhen your Monday rolls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Learn to Code: Python on 5/21</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Codeup Dallas Joins Career Day</td>\n",
       "      <td>Last week our Codeup Dallas team participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Project Quest Info Session: IT Jumpstart on Ma...</td>\n",
       "      <td>Join our grant partner Project Quest as they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>From Bootcamp to Bootcamp | A Military Appreci...</td>\n",
       "      <td>In honor of Military Appreciation Month, join ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our Acquisition of the Rackspace Cloud Academy...</td>\n",
       "      <td>Just about a year ago on April 16th, 2021 we a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Learn to Code: HTML &amp; CSS on 4/30</td>\n",
       "      <td>HTML &amp; CSS are the design building blocks of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Learn to Code: Python Workshop on 4/23</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "1   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "2   Is Our Cloud Administration Program Right for ...   \n",
       "3   5 Reasons To Attend Our New Cloud Administrati...   \n",
       "4   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "5   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "6   In-Person Workshop: Learn to Code – JavaScript...   \n",
       "7   In-Person Workshop: Learn to Code – Python on ...   \n",
       "8   Free JavaScript Workshop at Codeup Dallas on 6/28   \n",
       "9   Is Our Cloud Administration Program Right for ...   \n",
       "10                                PRIDE in Tech Panel   \n",
       "11  Inclusion at Codeup During Pride Month (and Al...   \n",
       "12                   Mental Health First Aid Training   \n",
       "13  Codeup Dallas: How to Succeed at a Coding Boot...   \n",
       "14  5 Reasons To Attend Our New Cloud Administrati...   \n",
       "15                      Learn to Code: Python on 5/21   \n",
       "16                     Codeup Dallas Joins Career Day   \n",
       "17  Project Quest Info Session: IT Jumpstart on Ma...   \n",
       "18  From Bootcamp to Bootcamp | A Military Appreci...   \n",
       "19  Our Acquisition of the Rackspace Cloud Academy...   \n",
       "20                  Learn to Code: HTML & CSS on 4/30   \n",
       "21             Learn to Code: Python Workshop on 4/23   \n",
       "\n",
       "                                              content  \n",
       "0   Have you been considering a career in Cloud Ad...  \n",
       "1   If you are interested in embarking on a career...  \n",
       "2   Changing careers can be scary. The first thing...  \n",
       "3   Come Work In The Cloud\\nWhen your Monday rolls...  \n",
       "4   Have you been considering a career in Cloud Ad...  \n",
       "5   If you are interested in embarking on a career...  \n",
       "6   Join us for our live in-person JavaScript cras...  \n",
       "7   According to LinkedIn, the “#1 Most Promising ...  \n",
       "8   Event Info: \\nLocation – Codeup Dallas\\nTime –...  \n",
       "9   Changing careers can be scary. The first thing...  \n",
       "10  In celebration of PRIDE month, join our Codeup...  \n",
       "11  Happy Pride Month! Pride Month is a dedicated ...  \n",
       "12  As a student of Codeup, going through a massiv...  \n",
       "13  This event is the perfect opportunity for peop...  \n",
       "14  Come Work In The Cloud\\nWhen your Monday rolls...  \n",
       "15  According to LinkedIn, the “#1 Most Promising ...  \n",
       "16  Last week our Codeup Dallas team participated ...  \n",
       "17  Join our grant partner Project Quest as they d...  \n",
       "18  In honor of Military Appreciation Month, join ...  \n",
       "19  Just about a year ago on April 16th, 2021 we a...  \n",
       "20  HTML & CSS are the design building blocks of a...  \n",
       "21  According to LinkedIn, the “#1 Most Promising ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = acquire.get_blog_articles_data()\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b0eb90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.rename(columns={'content': 'original'}, inplace=True)\n",
    "codeup_df.rename(columns={'content': 'original'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769bff75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter wins bid to fast-track trial over Musk...</td>\n",
       "      <td>Twitter's lawsuit seeking to hold Elon Musk to...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lyricist Jaani injured in accident in Punjab, ...</td>\n",
       "      <td>Renowned Punjabi lyricist and music composer J...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rare twin giraffes born in Kenya, pics surface</td>\n",
       "      <td>Rare twin giraffes have been born at Nairobi N...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She borrowed mother's stole: Father of girl al...</td>\n",
       "      <td>The father of a 17-year-old who claimed she wa...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi-bound flight rejected take-off due to do...</td>\n",
       "      <td>A flight was rejected take-off from Leh due to...</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>NIA raids 3 Andhra locations in Chhattisgarh N...</td>\n",
       "      <td>National Investigation Agency (NIA) conducted ...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Strikes on Syria will destabilise Middle East:...</td>\n",
       "      <td>Iranian Supreme Leader Ayatollah Ali Khamenei ...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Turkey court upholds exit from key European tr...</td>\n",
       "      <td>A Turkish court ruled on Tuesday that Presiden...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>France probes origins of wildfire, 1 detained</td>\n",
       "      <td>French investigators detained a 39-year-old ma...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Woman sleepwalks through window in Germany, fa...</td>\n",
       "      <td>An 18-year-old woman in Germany's Neubrandenbu...</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Twitter wins bid to fast-track trial over Musk...   \n",
       "1    Lyricist Jaani injured in accident in Punjab, ...   \n",
       "2       Rare twin giraffes born in Kenya, pics surface   \n",
       "3    She borrowed mother's stole: Father of girl al...   \n",
       "4    Delhi-bound flight rejected take-off due to do...   \n",
       "..                                                 ...   \n",
       "295  NIA raids 3 Andhra locations in Chhattisgarh N...   \n",
       "296  Strikes on Syria will destabilise Middle East:...   \n",
       "297  Turkey court upholds exit from key European tr...   \n",
       "298      France probes origins of wildfire, 1 detained   \n",
       "299  Woman sleepwalks through window in Germany, fa...   \n",
       "\n",
       "                                              original    category  \n",
       "0    Twitter's lawsuit seeking to hold Elon Musk to...    national  \n",
       "1    Renowned Punjabi lyricist and music composer J...    national  \n",
       "2    Rare twin giraffes have been born at Nairobi N...    national  \n",
       "3    The father of a 17-year-old who claimed she wa...    national  \n",
       "4    A flight was rejected take-off from Leh due to...    national  \n",
       "..                                                 ...         ...  \n",
       "295  National Investigation Agency (NIA) conducted ...  automobile  \n",
       "296  Iranian Supreme Leader Ayatollah Ali Khamenei ...  automobile  \n",
       "297  A Turkish court ruled on Tuesday that Presiden...  automobile  \n",
       "298  French investigators detained a 39-year-old ma...  automobile  \n",
       "299  An 18-year-old woman in Germany's Neubrandenbu...  automobile  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4be04b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>Have you been considering a career in Cloud Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>If you are interested in embarking on a career...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is Our Cloud Administration Program Right for ...</td>\n",
       "      <td>Changing careers can be scary. The first thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 Reasons To Attend Our New Cloud Administrati...</td>\n",
       "      <td>Come Work In The Cloud\\nWhen your Monday rolls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>Have you been considering a career in Cloud Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>If you are interested in embarking on a career...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In-Person Workshop: Learn to Code – JavaScript...</td>\n",
       "      <td>Join us for our live in-person JavaScript cras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>In-Person Workshop: Learn to Code – Python on ...</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Free JavaScript Workshop at Codeup Dallas on 6/28</td>\n",
       "      <td>Event Info: \\nLocation – Codeup Dallas\\nTime –...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is Our Cloud Administration Program Right for ...</td>\n",
       "      <td>Changing careers can be scary. The first thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PRIDE in Tech Panel</td>\n",
       "      <td>In celebration of PRIDE month, join our Codeup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Inclusion at Codeup During Pride Month (and Al...</td>\n",
       "      <td>Happy Pride Month! Pride Month is a dedicated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mental Health First Aid Training</td>\n",
       "      <td>As a student of Codeup, going through a massiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Codeup Dallas: How to Succeed at a Coding Boot...</td>\n",
       "      <td>This event is the perfect opportunity for peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5 Reasons To Attend Our New Cloud Administrati...</td>\n",
       "      <td>Come Work In The Cloud\\nWhen your Monday rolls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Learn to Code: Python on 5/21</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Codeup Dallas Joins Career Day</td>\n",
       "      <td>Last week our Codeup Dallas team participated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Project Quest Info Session: IT Jumpstart on Ma...</td>\n",
       "      <td>Join our grant partner Project Quest as they d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>From Bootcamp to Bootcamp | A Military Appreci...</td>\n",
       "      <td>In honor of Military Appreciation Month, join ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Our Acquisition of the Rackspace Cloud Academy...</td>\n",
       "      <td>Just about a year ago on April 16th, 2021 we a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Learn to Code: HTML &amp; CSS on 4/30</td>\n",
       "      <td>HTML &amp; CSS are the design building blocks of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Learn to Code: Python Workshop on 4/23</td>\n",
       "      <td>According to LinkedIn, the “#1 Most Promising ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "1   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "2   Is Our Cloud Administration Program Right for ...   \n",
       "3   5 Reasons To Attend Our New Cloud Administrati...   \n",
       "4   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "5   What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "6   In-Person Workshop: Learn to Code – JavaScript...   \n",
       "7   In-Person Workshop: Learn to Code – Python on ...   \n",
       "8   Free JavaScript Workshop at Codeup Dallas on 6/28   \n",
       "9   Is Our Cloud Administration Program Right for ...   \n",
       "10                                PRIDE in Tech Panel   \n",
       "11  Inclusion at Codeup During Pride Month (and Al...   \n",
       "12                   Mental Health First Aid Training   \n",
       "13  Codeup Dallas: How to Succeed at a Coding Boot...   \n",
       "14  5 Reasons To Attend Our New Cloud Administrati...   \n",
       "15                      Learn to Code: Python on 5/21   \n",
       "16                     Codeup Dallas Joins Career Day   \n",
       "17  Project Quest Info Session: IT Jumpstart on Ma...   \n",
       "18  From Bootcamp to Bootcamp | A Military Appreci...   \n",
       "19  Our Acquisition of the Rackspace Cloud Academy...   \n",
       "20                  Learn to Code: HTML & CSS on 4/30   \n",
       "21             Learn to Code: Python Workshop on 4/23   \n",
       "\n",
       "                                             original  \n",
       "0   Have you been considering a career in Cloud Ad...  \n",
       "1   If you are interested in embarking on a career...  \n",
       "2   Changing careers can be scary. The first thing...  \n",
       "3   Come Work In The Cloud\\nWhen your Monday rolls...  \n",
       "4   Have you been considering a career in Cloud Ad...  \n",
       "5   If you are interested in embarking on a career...  \n",
       "6   Join us for our live in-person JavaScript cras...  \n",
       "7   According to LinkedIn, the “#1 Most Promising ...  \n",
       "8   Event Info: \\nLocation – Codeup Dallas\\nTime –...  \n",
       "9   Changing careers can be scary. The first thing...  \n",
       "10  In celebration of PRIDE month, join our Codeup...  \n",
       "11  Happy Pride Month! Pride Month is a dedicated ...  \n",
       "12  As a student of Codeup, going through a massiv...  \n",
       "13  This event is the perfect opportunity for peop...  \n",
       "14  Come Work In The Cloud\\nWhen your Monday rolls...  \n",
       "15  According to LinkedIn, the “#1 Most Promising ...  \n",
       "16  Last week our Codeup Dallas team participated ...  \n",
       "17  Join our grant partner Project Quest as they d...  \n",
       "18  In honor of Military Appreciation Month, join ...  \n",
       "19  Just about a year ago on April 16th, 2021 we a...  \n",
       "20  HTML & CSS are the design building blocks of a...  \n",
       "21  According to LinkedIn, the “#1 Most Promising ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca7d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[], stem=True, lemmatize=True):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords,\n",
    "                                  extra_words=extra_words,\n",
    "                                  exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d74d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords,\n",
    "                                  extra_words=extra_words,\n",
    "                                  exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aefde7f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Ray/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/Ray/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprep_article_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mha\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mprep_article_data\u001b[0;34m(df, column, extra_words, exclude_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprep_article_data\u001b[39m(df, column, extra_words\u001b[38;5;241m=\u001b[39m[], exclude_words\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    This function take in a df and the string name for a text column with \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    option to pass lists for extra_words and exclude_words and\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    returns a df with the text article title, original text, stemmed text,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_clean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mextra_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mexclude_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(stem)\n\u001b[1;32m     16\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(lemmatize)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:138\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(string, extra_words, exclude_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis function takes in a string, optional extra_words and exclude_words parameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mwith default empty lists and returns a string.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# assign our stopwords from nltk into stopword_list\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m stopword_list \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# utilizing set casting, i will remove any excluded stopwords\u001b[39;00m\n\u001b[1;32m      9\u001b[0m stopword_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopword_list) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude_words)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Ray/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "prep_article_data(news_df, 'original', extra_words = ['ha'], exclude_words = ['no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec6693e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Ray/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/Ray/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprep_article_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodeup_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mha\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mprep_article_data\u001b[0;34m(df, column, extra_words, exclude_words)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprep_article_data\u001b[39m(df, column, extra_words\u001b[38;5;241m=\u001b[39m[], exclude_words\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    This function take in a df and the string name for a text column with \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    option to pass lists for extra_words and exclude_words and\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    returns a df with the text article title, original text, stemmed text,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_clean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mextra_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mexclude_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(stem)\n\u001b[1;32m     16\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(lemmatize)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:138\u001b[0m, in \u001b[0;36mApply.__init__.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(string, extra_words, exclude_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis function takes in a string, optional extra_words and exclude_words parameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mwith default empty lists and returns a string.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# assign our stopwords from nltk into stopword_list\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m stopword_list \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# utilizing set casting, i will remove any excluded stopwords\u001b[39;00m\n\u001b[1;32m      9\u001b[0m stopword_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopword_list) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude_words)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Ray/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "prep_article_data(codeup_df, 'original', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd8bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
